#!/usr/bin/env python3
"""
Routing Cycle Detector
Finds the longest routing cycle in a claim routing file.
Downloads data from URL before processing.
"""

import sys
import os
import argparse
import re
import urllib.request
import urllib.error
import logging
from datetime import datetime
from collections import defaultdict


# Global variables for run tracking
RUN_TIMESTAMP = None
RUN_FOLDER = None
LOGGER = None


def setup_logging(results_base="results"):
    """
    Setup logging with both console and file handlers.
    Creates the run folder with timestamp.
    """
    global RUN_TIMESTAMP, RUN_FOLDER, LOGGER
    
    # Generate timestamp for this run
    RUN_TIMESTAMP = datetime.now()
    timestamp_str = RUN_TIMESTAMP.strftime("%Y%m%d_%H%M%S")
    
    # Create results folder structure
    RUN_FOLDER = os.path.join(results_base, f"run_{timestamp_str}")
    os.makedirs(RUN_FOLDER, exist_ok=True)
    
    # Setup logger
    LOGGER = logging.getLogger("RoutingCycleDetector")
    LOGGER.setLevel(logging.DEBUG)
    
    # Clear any existing handlers
    LOGGER.handlers.clear()
    
    # Console handler (INFO level)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_format)
    
    # File handler (DEBUG level - more detailed)
    log_file = os.path.join(RUN_FOLDER, "run.log")
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_format = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
    file_handler.setFormatter(file_format)
    
    LOGGER.addHandler(console_handler)
    LOGGER.addHandler(file_handler)
    
    LOGGER.info(f"Logging initialized. Run folder: {RUN_FOLDER}")
    LOGGER.debug(f"Log file: {log_file}")
    
    return LOGGER


def save_results(claim_id, status_code, cycle_length, data_url, data_filename, local_file=None):
    """
    Save the solution and metadata to the run folder.
    """
    global RUN_TIMESTAMP, RUN_FOLDER, LOGGER
    
    # Save solution.txt
    solution_path = os.path.join(RUN_FOLDER, "solution.txt")
    solution_content = f"{claim_id},{status_code},{cycle_length}"
    
    with open(solution_path, 'w', encoding='utf-8') as f:
        f.write(solution_content)
    
    LOGGER.info(f"Solution saved to: {solution_path}")
    LOGGER.debug(f"Solution content: {solution_content}")
    
    # Save solution_metadata.yml
    metadata_path = os.path.join(RUN_FOLDER, "solution_metadata.yml")
    
    # Determine data source type
    if local_file:
        data_source_section = f"""data_source:
  type: "local_file"
  local_path: "{local_file}"
  filename: "{data_filename}"""
    else:
        data_source_section = f"""data_source:
  type: "url"
  url: "{data_url}"
  filename: "{data_filename}"""
    
    metadata_content = f"""# Solution Metadata
# Generated by Routing Cycle Detector

run_date: "{RUN_TIMESTAMP.strftime('%Y-%m-%d')}"
run_time: "{RUN_TIMESTAMP.strftime('%H:%M:%S')}"
run_timestamp: "{RUN_TIMESTAMP.isoformat()}"

{data_source_section}

result:
  claim_id: "{claim_id}"
  status_code: "{status_code}"
  cycle_length: {cycle_length}
"""
    
    with open(metadata_path, 'w', encoding='utf-8') as f:
        f.write(metadata_content)
    
    LOGGER.info(f"Metadata saved to: {metadata_path}")
    
    return solution_path, metadata_path


def extract_google_drive_id(url):
    """Extract file ID from Google Drive URL."""
    patterns = [
        r'/file/d/([a-zA-Z0-9_-]+)',
        r'id=([a-zA-Z0-9_-]+)',
        r'/d/([a-zA-Z0-9_-]+)',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None


def download_file(url, dest_folder="data"):
    """
    Download file from URL (supports Google Drive links).
    Returns the path to the downloaded file.
    """
    global LOGGER
    
    # Create destination folder if it doesn't exist
    os.makedirs(dest_folder, exist_ok=True)
    LOGGER.debug(f"Destination folder created/verified: {dest_folder}")
    
    # Check if it's a Google Drive URL
    if "drive.google.com" in url:
        file_id = extract_google_drive_id(url)
        if not file_id:
            LOGGER.error("Could not extract file ID from Google Drive URL")
            raise ValueError("Could not extract file ID from Google Drive URL")
        
        # Use the drive.usercontent.google.com endpoint for large files
        download_url = f"https://drive.usercontent.google.com/download?id={file_id}&export=download&confirm=t"
        filename = "large_input_v1.txt"
        LOGGER.debug(f"Google Drive file ID extracted: {file_id}")
    else:
        download_url = url
        filename = os.path.basename(url.split('?')[0]) or "downloaded_data.txt"
    
    dest_path = os.path.join(dest_folder, filename)
    
    LOGGER.info(f"Downloading from: {download_url}")
    LOGGER.info(f"Saving to: {dest_path}")
    
    try:
        # Create request with headers
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        request = urllib.request.Request(download_url, headers=headers)
        LOGGER.debug("HTTP request created with custom headers")
        
        with urllib.request.urlopen(request, timeout=300) as response:
            total_size = response.headers.get('Content-Length')
            if total_size:
                total_size = int(total_size)
                LOGGER.info(f"File size: {total_size / (1024*1024):.2f} MB")
            
            # Download with progress indicator
            downloaded = 0
            chunk_size = 1024 * 1024  # 1MB chunks
            last_log_percent = 0
            
            with open(dest_path, 'wb') as out_file:
                while True:
                    chunk = response.read(chunk_size)
                    if not chunk:
                        break
                    out_file.write(chunk)
                    downloaded += len(chunk)
                    if total_size:
                        progress = (downloaded / total_size) * 100
                        print(f"\rProgress: {progress:.1f}% ({downloaded / (1024*1024):.2f} MB)", end='', flush=True)
                        # Log every 10%
                        if int(progress / 10) > last_log_percent:
                            last_log_percent = int(progress / 10)
                            LOGGER.debug(f"Download progress: {progress:.1f}%")
                    else:
                        print(f"\rDownloaded: {downloaded / (1024*1024):.2f} MB", end='', flush=True)
            
            print()  # New line after progress
        
        # Verify file was downloaded
        if os.path.exists(dest_path) and os.path.getsize(dest_path) > 0:
            file_size = os.path.getsize(dest_path)
            LOGGER.info(f"Download complete! File size: {file_size / (1024*1024):.2f} MB")
            
            # Quick validation - check if it's HTML (error page)
            with open(dest_path, 'r', encoding='utf-8', errors='ignore') as f:
                first_line = f.readline()
                if first_line.strip().startswith('<!DOCTYPE html>') or first_line.strip().startswith('<html'):
                    LOGGER.error("Download failed - received HTML page instead of data file")
                    raise RuntimeError("Download failed - received HTML page instead of data file")
            
            LOGGER.debug("File validation passed - not an HTML error page")
            return dest_path, filename
        else:
            LOGGER.error("Download failed - file is empty or doesn't exist")
            raise RuntimeError("Download failed - file is empty or doesn't exist")
            
    except urllib.error.URLError as e:
        LOGGER.error(f"Failed to download file: {e}")
        raise RuntimeError(f"Failed to download file: {e}")


def find_longest_cycle_in_graph(graph):
    """
    Find the longest simple cycle in a directed graph using DFS.
    Returns the length of the longest cycle found.
    """
    if not graph:
        return 0
    
    nodes = set(graph.keys())
    for destinations in graph.values():
        nodes.update(destinations)
    
    longest = 0
    
    def dfs(start, current, visited, path_length):
        nonlocal longest
        
        if current not in graph:
            return
        
        for neighbor in graph[current]:
            if neighbor == start and path_length >= 1:
                # Found a cycle back to start
                longest = max(longest, path_length + 1)
            elif neighbor not in visited:
                visited.add(neighbor)
                dfs(start, neighbor, visited, path_length + 1)
                visited.remove(neighbor)
    
    # Try starting from each node
    for start_node in graph:
        visited = {start_node}
        dfs(start_node, start_node, visited, 0)
    
    return longest


def process_file(filepath):
    """
    Stream the file and build graphs per (claim_id, status_code).
    Returns dictionary mapping (claim_id, status_code) -> graph
    """
    global LOGGER
    
    # Dictionary: (claim_id, status_code) -> {source -> [destinations]}
    graphs = defaultdict(lambda: defaultdict(list))
    line_count = 0
    valid_lines = 0
    
    LOGGER.info(f"Processing file: {filepath}")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line_count += 1
            line = line.strip()
            if not line:
                continue
            
            parts = line.split('|')
            if len(parts) != 4:
                continue
            
            valid_lines += 1
            source, dest, claim_id, status_code = parts
            key = (claim_id, status_code)
            graphs[key][source].append(dest)
            
            # Log progress every 1 million lines
            if line_count % 1000000 == 0:
                LOGGER.debug(f"Processed {line_count:,} lines...")
    
    LOGGER.info(f"File processing complete: {line_count:,} total lines, {valid_lines:,} valid entries")
    LOGGER.info(f"Found {len(graphs)} unique (claim_id, status_code) combinations")
    
    return graphs


def find_longest_routing_cycle(filepath):
    """
    Main function to find the longest routing cycle.
    Returns (claim_id, status_code, cycle_length)
    """
    global LOGGER
    
    graphs = process_file(filepath)
    
    best_claim_id = None
    best_status_code = None
    best_length = 0
    graphs_processed = 0
    
    LOGGER.info("Searching for longest routing cycle...")
    
    for (claim_id, status_code), graph in graphs.items():
        cycle_length = find_longest_cycle_in_graph(dict(graph))
        graphs_processed += 1
        
        if cycle_length > best_length:
            best_length = cycle_length
            best_claim_id = claim_id
            best_status_code = status_code
            LOGGER.debug(f"New best cycle found: claim_id={claim_id}, status_code={status_code}, length={cycle_length}")
        
        # Log progress every 10000 graphs
        if graphs_processed % 10000 == 0:
            LOGGER.debug(f"Analyzed {graphs_processed:,} graphs...")
    
    LOGGER.info(f"Cycle search complete: analyzed {graphs_processed:,} graphs")
    
    if best_claim_id:
        LOGGER.info(f"Longest cycle found: claim_id={best_claim_id}, status_code={best_status_code}, length={best_length}")
    else:
        LOGGER.warning("No cycles found in the data")
    
    return best_claim_id, best_status_code, best_length


def main():
    global LOGGER
    
    parser = argparse.ArgumentParser(
        description="Routing Cycle Detector - Downloads data and finds the longest routing cycle"
    )
    parser.add_argument(
        "data_url",
        nargs="?",
        default=None,
        help="URL to download the input data file (supports Google Drive links)"
    )
    parser.add_argument(
        "--local-file",
        type=str,
        default=None,
        help="Path to a local data file to use instead of downloading"
    )
    parser.add_argument(
        "--dest-folder",
        default="data",
        help="Destination folder for downloaded file (default: data)"
    )
    parser.add_argument(
        "--results-folder",
        default="results",
        help="Base folder for results (default: results)"
    )
    parser.add_argument(
        "--skip-download",
        action="store_true",
        help="Skip download and use existing file in data folder"
    )
    
    args = parser.parse_args()
    
    # Validate arguments
    if not args.local_file and not args.data_url:
        parser.error("Either data_url or --local-file must be provided")
    
    try:
        # Initialize logging and create run folder
        setup_logging(args.results_folder)
        
        LOGGER.info("=" * 50)
        LOGGER.info("Routing Cycle Detector - Starting")
        LOGGER.info("=" * 50)
        
        # Determine data source
        data_url = args.data_url if args.data_url else "N/A (local file)"
        
        if args.local_file:
            LOGGER.info(f"Local file: {args.local_file}")
        else:
            LOGGER.info(f"Data URL: {args.data_url}")
        LOGGER.info(f"Data destination folder: {args.dest_folder}")
        LOGGER.info(f"Results folder: {RUN_FOLDER}")
        
        # Download the file or use existing/local
        LOGGER.info("")
        LOGGER.info("=== Phase 1: Loading Data ===")
        
        if args.local_file:
            # Use the specified local file
            if os.path.exists(args.local_file):
                filepath = args.local_file
                filename = os.path.basename(args.local_file)
                LOGGER.info(f"Using local file: {filepath}")
                LOGGER.info(f"File size: {os.path.getsize(filepath) / (1024*1024):.2f} MB")
            else:
                LOGGER.error(f"Local file not found: {args.local_file}")
                raise FileNotFoundError(f"Local file not found: {args.local_file}")
        elif args.skip_download:
            # Try to find existing file in data folder
            existing_file = os.path.join(args.dest_folder, "large_input_v1.txt")
            if os.path.exists(existing_file):
                filepath = existing_file
                filename = "large_input_v1.txt"
                LOGGER.info(f"Skipping download, using existing file: {filepath}")
            else:
                LOGGER.error(f"No existing file found at: {existing_file}")
                raise FileNotFoundError(f"No existing file at {existing_file}. Remove --skip-download to download.")
        else:
            filepath, filename = download_file(args.data_url, args.dest_folder)
        
        # Process the downloaded file
        LOGGER.info("")
        LOGGER.info("=== Phase 2: Processing Data ===")
        claim_id, status_code, cycle_length = find_longest_routing_cycle(filepath)
        
        if claim_id is not None:
            LOGGER.info("")
            LOGGER.info("=== Phase 3: Saving Results ===")
            save_results(claim_id, status_code, cycle_length, data_url, filename, args.local_file)
            
            LOGGER.info("")
            LOGGER.info("=" * 50)
            LOGGER.info("FINAL RESULT")
            LOGGER.info("=" * 50)
            LOGGER.info(f"Claim ID: {claim_id}")
            LOGGER.info(f"Status Code: {status_code}")
            LOGGER.info(f"Cycle Length: {cycle_length}")
            LOGGER.info(f"Output: {claim_id},{status_code},{cycle_length}")
            LOGGER.info(f"Results saved to: {RUN_FOLDER}")
            LOGGER.info("=" * 50)
            
            # Also print the result to stdout for compatibility
            print(f"\n{claim_id},{status_code},{cycle_length}")
        else:
            LOGGER.error("No cycles found")
            sys.exit(1)
            
    except Exception as e:
        if LOGGER:
            LOGGER.error(f"Error: {e}", exc_info=True)
        else:
            print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
